{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers\n%pip install -U datasets\n%pip install -U accelerate \n%pip install -U peft\n%pip install -U trl\n%pip install -U bitsandbytes\n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-08-18T14:42:02.420747Z","iopub.execute_input":"2024-08-18T14:42:02.420984Z","iopub.status.idle":"2024-08-18T14:44:06.367888Z","shell.execute_reply.started":"2024-08-18T14:42:02.420961Z","shell.execute_reply":"2024-08-18T14:44:06.366782Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/A7medM0sta/Transformer_From_Scratch_Translation.git","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:23:32.512262Z","iopub.execute_input":"2024-08-19T12:23:32.513149Z","iopub.status.idle":"2024-08-19T12:23:34.256434Z","shell.execute_reply.started":"2024-08-19T12:23:32.513112Z","shell.execute_reply":"2024-08-19T12:23:34.255508Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'Transformer_From_Scratch_Translation'...\nremote: Enumerating objects: 89, done.\u001b[K\nremote: Counting objects: 100% (89/89), done.\u001b[K\nremote: Compressing objects: 100% (76/76), done.\u001b[K\nremote: Total 89 (delta 24), reused 62 (delta 7), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (89/89), 3.03 MiB | 12.02 MiB/s, done.\nResolving deltas: 100% (24/24), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%load_ext tensorboard\n!pip install datasets\n!pip install tokenizers\n!pip install torchmetrics","metadata":{"execution":{"iopub.status.busy":"2024-08-19T12:24:20.897003Z","iopub.execute_input":"2024-08-19T12:24:20.897410Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.13.3)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.1.1)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.23.5)\nRequirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.0.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.9.0)\nRequirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (21.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.6.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.1->lightning-utilities>=0.8.0->torchmetrics) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd Transformer_From_Scratch_Translation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,)\n\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"VLzgZ14X_rMs","execution":{"iopub.status.busy":"2024-08-18T14:44:06.370126Z","iopub.execute_input":"2024-08-18T14:44:06.370472Z","iopub.status.idle":"2024-08-18T14:44:18.858717Z","shell.execute_reply.started":"2024-08-18T14:44:06.370441Z","shell.execute_reply":"2024-08-18T14:44:18.857929Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T14:44:18.859714Z","iopub.execute_input":"2024-08-18T14:44:18.860232Z","iopub.status.idle":"2024-08-18T14:44:19.251237Z","shell.execute_reply.started":"2024-08-18T14:44:18.860207Z","shell.execute_reply":"2024-08-18T14:44:19.250322Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"wb_token = user_secrets.get_secret(\"Secret_key\") # wandb\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"id":"na9CAoHC5gM9","execution":{"iopub.status.busy":"2024-08-18T14:44:19.253587Z","iopub.execute_input":"2024-08-18T14:44:19.253870Z","iopub.status.idle":"2024-08-18T14:44:37.888109Z","shell.execute_reply.started":"2024-08-18T14:44:19.253845Z","shell.execute_reply":"2024-08-18T14:44:37.886609Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahmed-mostafa22200028\u001b[0m (\u001b[33mcrime\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240818_144421-wxcvdcs9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/crime/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/wxcvdcs9' target=\"_blank\">sparkling-pond-9</a></strong> to <a href='https://wandb.ai/crime/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/crime/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/crime/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/crime/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/wxcvdcs9' target=\"_blank\">https://wandb.ai/crime/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/wxcvdcs9</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\ndataset_name = \"ruslanmv/ai-medical-chatbot\"\nnew_model = \"llama-3-8b-chat-doctor\"","metadata":{"execution":{"iopub.status.busy":"2024-08-18T14:44:37.889541Z","iopub.execute_input":"2024-08-18T14:44:37.889909Z","iopub.status.idle":"2024-08-18T14:44:37.898602Z","shell.execute_reply.started":"2024-08-18T14:44:37.889871Z","shell.execute_reply":"2024-08-18T14:44:37.897465Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Set torch dtype and attention implementation\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-08-18T14:44:37.899596Z","iopub.execute_input":"2024-08-18T14:44:37.899924Z","iopub.status.idle":"2024-08-18T14:44:38.118120Z","shell.execute_reply.started":"2024-08-18T14:44:37.899900Z","shell.execute_reply":"2024-08-18T14:44:38.116508Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"id":"StJKGiDDHzdk","outputId":"871214ba-6c30-4ecf-ac68-550f296b7ef6","execution":{"iopub.status.busy":"2024-08-18T14:44:38.122357Z","iopub.execute_input":"2024-08-18T14:44:38.122654Z","iopub.status.idle":"2024-08-18T14:46:28.523761Z","shell.execute_reply.started":"2024-08-18T14:44:38.122629Z","shell.execute_reply":"2024-08-18T14:46:28.522610Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"652f622eb7764f7e88ee978e75830f65"}},"metadata":{}}]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T14:46:28.525020Z","iopub.execute_input":"2024-08-18T14:46:28.525890Z","iopub.status.idle":"2024-08-18T14:46:29.653257Z","shell.execute_reply.started":"2024-08-18T14:46:28.525848Z","shell.execute_reply":"2024-08-18T14:46:29.651569Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=42).select(range(2000))\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc= 4,)\ndataset","metadata":{"id":"XzF2UjPvTBag","outputId":"3733e45f-605e-4564-88c7-368c9c5bf9cd","execution":{"iopub.status.busy":"2024-08-18T14:46:34.629921Z","iopub.execute_input":"2024-08-18T14:46:34.631074Z","iopub.status.idle":"2024-08-18T14:46:35.931248Z","shell.execute_reply.started":"2024-08-18T14:46:34.631034Z","shell.execute_reply":"2024-08-18T14:46:35.930216Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Description', 'Patient', 'Doctor', 'text'],\n    num_rows: 2000\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-08-18T14:46:35.932537Z","iopub.execute_input":"2024-08-18T14:46:35.932905Z","iopub.status.idle":"2024-08-18T14:46:35.946197Z","shell.execute_reply.started":"2024-08-18T14:46:35.932871Z","shell.execute_reply":"2024-08-18T14:46:35.945203Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>user\\nHi doctor. My GGT reading in my liver function was abou 850; but a CT scan revealed no scar or damage to my liver and every other about the liver appeared normal except that it was mildly enlarged leading to a conclusion of mild hepatomegaly. I have been placed on Livolin Forte for 1 month and within 2 weeks of intake, my GGT dropped to 650. Every other parameter in liver has dropped to normal. What this portend for me and what is the hope of my liver normalising?<|im_end|>\\n<|im_start|>assistant\\nthank you for posting query.increased GGT presentation but with Insufficient history.increased ggt without increased liver enzymes maybe due to 1. alcohol abuse     2. gall bladder pathology    3. certain medicationhave you ruled out all causes. underwent blood tests and radiological examination (ct and ultrasound of abdomen). if not, do it asap.Livolin fort is safe to use.further advice:- abstinence from \"Alcohol and drugs\" - LOW fat diet should be followed- vegetables should be ingested daily- use lemon juice (lemonade) once in a day- walk 30 to 40 minutes everyday.- or swimming is alternative to walking-\"recheck GGT enzyme after 6 to 8 weeks\".if any further questions, feel free to ask.Health professionals aim to diagnose properly and manage patients according to their limited knowledge. Cure is blessed by the ONE who Created us, whose power and knowledge is unlimited .wish you good health.regards,Dr Tayyab Malik<|im_end|>\\n'"},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T14:46:35.947544Z","iopub.execute_input":"2024-08-18T14:46:35.948293Z","iopub.status.idle":"2024-08-18T14:46:35.962866Z","shell.execute_reply.started":"2024-08-18T14:46:35.948258Z","shell.execute_reply":"2024-08-18T14:46:35.961898Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=2,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"id":"peOnLAAhS0y1","execution":{"iopub.status.busy":"2024-08-18T14:49:27.973485Z","iopub.execute_input":"2024-08-18T14:49:27.973854Z","iopub.status.idle":"2024-08-18T14:49:28.010309Z","shell.execute_reply.started":"2024-08-18T14:49:27.973824Z","shell.execute_reply":"2024-08-18T14:49:28.009500Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T14:49:28.875249Z","iopub.execute_input":"2024-08-18T14:49:28.876072Z","iopub.status.idle":"2024-08-18T14:49:30.577634Z","shell.execute_reply.started":"2024-08-18T14:49:28.876038Z","shell.execute_reply":"2024-08-18T14:49:30.576205Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a5435e8a2e4fb582ed1875aa00f7b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7785e8aa64894da8a15a659e832a84e0"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T14:49:30.579493Z","iopub.execute_input":"2024-08-18T14:49:30.580098Z","iopub.status.idle":"2024-08-18T16:28:54.061687Z","shell.execute_reply.started":"2024-08-18T14:49:30.580064Z","shell.execute_reply":"2024-08-18T16:28:54.060700Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2700' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2700/2700 1:39:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>540</td>\n      <td>2.522600</td>\n      <td>2.481651</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>1.948300</td>\n      <td>2.505425</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>1.682400</td>\n      <td>2.479209</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>1.281300</td>\n      <td>2.789191</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.665800</td>\n      <td>2.773987</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2700, training_loss=1.8522976643561075, metrics={'train_runtime': 5962.3227, 'train_samples_per_second': 0.906, 'train_steps_per_second': 0.453, 'total_flos': 5.585827747221504e+16, 'train_loss': 1.8522976643561075, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"id":"nKgZBEGVS5a2","execution":{"iopub.status.busy":"2024-08-18T16:30:12.010650Z","iopub.execute_input":"2024-08-18T16:30:12.011022Z","iopub.status.idle":"2024-08-18T16:30:22.064559Z","shell.execute_reply.started":"2024-08-18T16:30:12.010990Z","shell.execute_reply":"2024-08-18T16:30:22.063767Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁▂▁██</td></tr><tr><td>eval/runtime</td><td>▃█▄▁▃</td></tr><tr><td>eval/samples_per_second</td><td>▆▁▅█▇</td></tr><tr><td>eval/steps_per_second</td><td>▆▁▅█▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▃▃▂▂▂▁▁▄▁▁▂▂▁▂▂▃▃▃▂▃▄▂▂▂▃▅▃▄▃▆█▃▄▅▆▂▄▆▆</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▇▅█▆▇▇▆▄▆▆▆▇▅▇▄▄▅▆▆▄▅▅▄▄▄▃▄▃▃▁▃▂▁▄▃▂▃▂▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.77399</td></tr><tr><td>eval/runtime</td><td>89.2592</td></tr><tr><td>eval/samples_per_second</td><td>2.241</td></tr><tr><td>eval/steps_per_second</td><td>2.241</td></tr><tr><td>total_flos</td><td>5.585827747221504e+16</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>2700</td></tr><tr><td>train/grad_norm</td><td>4.34468</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6658</td></tr><tr><td>train_loss</td><td>1.8523</td></tr><tr><td>train_runtime</td><td>5962.3227</td></tr><tr><td>train_samples_per_second</td><td>0.906</td></tr><tr><td>train_steps_per_second</td><td>0.453</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sparkling-pond-9</strong> at: <a href='https://wandb.ai/crime/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/wxcvdcs9' target=\"_blank\">https://wandb.ai/crime/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/wxcvdcs9</a><br/> View project at: <a href='https://wandb.ai/crime/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/crime/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240818_144421-wxcvdcs9/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:31:06.732547Z","iopub.execute_input":"2024-08-18T16:31:06.733439Z","iopub.status.idle":"2024-08-18T16:32:21.498712Z","shell.execute_reply.started":"2024-08-18T16:31:06.733398Z","shell.execute_reply":"2024-08-18T16:32:21.497677Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab67c244805a4e7b924f3e1dd7a4e94c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5503c1567e040bda52e45f63450fab9"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/A7m0d/llama-3-8b-chat-doctor/commit/bc3f481ce7d160b591b8be309b9a0322a8bdd945', commit_message='Upload model', commit_description='', oid='bc3f481ce7d160b591b8be309b9a0322a8bdd945', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"class ChatBot:\n    \"\"\"\n    A simple chatbot class that interacts with the user by taking queries and responding using a language model.\n    The chatbot continues to take user input until the user types 'quit' to exit.\n\n    Attributes:\n        tokenizer (object): The tokenizer to process input and output texts.\n        model (object): The language model used to generate responses.\n    \"\"\"\n\n    def __init__(self, tokenizer, model):\n        \"\"\"\n        Initializes the ChatBot with a tokenizer and a model.\n\n        Args:\n            tokenizer (object): The tokenizer to process input and output texts.\n            model (object): The language model used to generate responses.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.model = model\n\n    def chat(self):\n        \"\"\"\n        Starts the chatbot interaction with the user, taking queries and responding until the user types 'quit'.\n\n        The chatbot will generate and print responses based on user input.\n        \"\"\"\n        while True:\n            user_input = input(\"You: \")\n            if user_input.lower() == \"quit\":\n                print(\"Goodbye!\")\n                break\n\n            messages = [{\"role\": \"user\", \"content\": user_input}]\n            prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n            inputs = self.tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n            outputs = self.model.generate(**inputs, max_length=150, num_return_sequences=1)\n\n            text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            response = text.split(\"assistant\")[1].strip()\n\n            print(f\"Bot: {response}\")\n\n# Example usage:\n# from some_library import tokenizer, model\nbot = ChatBot(tokenizer, model)\nbot.chat()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:46:56.723797Z","iopub.execute_input":"2024-08-18T16:46:56.724128Z","iopub.status.idle":"2024-08-18T16:46:56.760172Z","shell.execute_reply.started":"2024-08-18T16:46:56.724103Z","shell.execute_reply":"2024-08-18T16:46:56.759073Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# from some_library import tokenizer, model\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m bot \u001b[38;5;241m=\u001b[39m ChatBot(\u001b[43mtokenizer\u001b[49m, model)\n\u001b[1;32m     48\u001b[0m bot\u001b[38;5;241m.\u001b[39mchat()\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"messages = [{\"role\": \"system\", \"content\": \"If you are a doctor, please answer the medical questions based on the patient's description.\"},\n    {\"role\": \"user\", \"content\": \"Hello, I am in the middle of a severe anxiety/panic attack. Could you help me?\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:32:21.500181Z","iopub.execute_input":"2024-08-18T16:32:21.500436Z","iopub.status.idle":"2024-08-18T16:32:42.916188Z","shell.execute_reply.started":"2024-08-18T16:32:21.500413Z","shell.execute_reply":"2024-08-18T16:32:42.915108Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-4.41.0-py3-none-any.whl (12.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.0)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.98.0)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\nCollecting gradio-client==1.3.0 (from gradio)\n  Downloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.24.5)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.12.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.23.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.0.2)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (9.5.0)\nCollecting pydantic>=2.0 (from gradio)\n  Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.9 (from gradio)\n  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0)\nCollecting ruff>=0.2.2 (from gradio)\n  Downloading ruff-0.6.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nCollecting tomlkit==0.12.0 (from gradio)\n  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\nCollecting typer<1.0,>=0.12 (from gradio)\n  Downloading typer-0.12.4-py3-none-any.whl (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.12.2)\nCollecting urllib3~=2.0 (from gradio)\n  Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.22.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (2023.9.0)\nRequirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (11.0.3)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.1.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2023.7.22)\nCollecting httpcore==1.* (from httpx>=0.24.1->gradio)\n  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.5.0)\nCollecting pydantic-core==2.20.1 (from pydantic>=2.0->gradio)\n  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.3)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.4.2)\nINFO: pip is looking at multiple versions of fastapi to determine which version is compatible with other requirements. This could take a while.\nCollecting fastapi (from gradio)\n  Downloading fastapi-0.112.1-py3-none-any.whl (93 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting starlette<0.39.0,>=0.37.2 (from fastapi->gradio)\n  Downloading starlette-0.38.2-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.1.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\nInstalling collected packages: urllib3, tomlkit, semantic-version, ruff, python-multipart, pydantic-core, httpcore, ffmpy, starlette, pydantic, httpx, typer, fastapi, gradio-client, gradio\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.15\n    Uninstalling urllib3-1.26.15:\n      Successfully uninstalled urllib3-1.26.15\n  Attempting uninstall: tomlkit\n    Found existing installation: tomlkit 0.12.1\n    Uninstalling tomlkit-0.12.1:\n      Successfully uninstalled tomlkit-0.12.1\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.6.3\n    Uninstalling pydantic_core-2.6.3:\n      Successfully uninstalled pydantic_core-2.6.3\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.27.0\n    Uninstalling starlette-0.27.0:\n      Successfully uninstalled starlette-0.27.0\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.9\n    Uninstalling pydantic-1.10.9:\n      Successfully uninstalled pydantic-1.10.9\n  Attempting uninstall: typer\n    Found existing installation: typer 0.9.0\n    Uninstalling typer-0.9.0:\n      Successfully uninstalled typer-0.9.0\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.98.0\n    Uninstalling fastapi-0.98.0:\n      Successfully uninstalled fastapi-0.98.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbotocore 1.31.17 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.2 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ngoogle-auth 2.20.0 requires urllib3<2.0, but you have urllib3 2.2.2 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.0.1 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\nspacy 3.6.1 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.4 which is incompatible.\nydata-profiling 4.3.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.8.2 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fastapi-0.112.1 ffmpy-0.4.0 gradio-4.41.0 gradio-client-1.3.0 httpcore-1.0.5 httpx-0.27.0 pydantic-2.3.0 pydantic-core-2.20.1 python-multipart-0.0.9 ruff-0.6.1 semantic-version-2.10.0 starlette-0.38.2 tomlkit-0.12.0 typer-0.12.4 urllib3-1.26.16\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gradio","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:44:09.126818Z","iopub.execute_input":"2024-08-18T16:44:09.127184Z","iopub.status.idle":"2024-08-18T16:44:45.353324Z","shell.execute_reply.started":"2024-08-18T16:44:09.127155Z","shell.execute_reply":"2024-08-18T16:44:45.352242Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-4.41.0-py3-none-any.whl (12.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.0)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.98.0)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\nCollecting gradio-client==1.3.0 (from gradio)\n  Downloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting huggingface-hub>=0.19.3 (from gradio)\n  Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.12.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.23.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.0.2)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (9.5.0)\nCollecting pydantic>=2.0 (from gradio)\n  Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.9 (from gradio)\n  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0)\nCollecting ruff>=0.2.2 (from gradio)\n  Downloading ruff-0.6.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nCollecting tomlkit==0.12.0 (from gradio)\n  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\nCollecting typer<1.0,>=0.12 (from gradio)\n  Downloading typer-0.12.4-py3-none-any.whl (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.6.3)\nCollecting urllib3~=2.0 (from gradio)\n  Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.22.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (2023.9.0)\nRequirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (11.0.3)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.1.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2023.7.22)\nCollecting httpcore==1.* (from httpx>=0.24.1->gradio)\n  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.5.0)\nCollecting pydantic-core==2.20.1 (from pydantic>=2.0->gradio)\n  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.3)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.4.2)\nINFO: pip is looking at multiple versions of fastapi to determine which version is compatible with other requirements. This could take a while.\nCollecting fastapi (from gradio)\n  Downloading fastapi-0.112.1-py3-none-any.whl (93 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting starlette<0.39.0,>=0.37.2 (from fastapi->gradio)\n  Downloading starlette-0.38.2-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting typing-extensions~=4.0 (from gradio)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.1.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\nInstalling collected packages: urllib3, typing-extensions, tomlkit, semantic-version, ruff, python-multipart, httpcore, ffmpy, starlette, pydantic-core, httpx, typer, pydantic, huggingface-hub, gradio-client, fastapi, gradio\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.15\n    Uninstalling urllib3-1.26.15:\n      Successfully uninstalled urllib3-1.26.15\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.6.3\n    Uninstalling typing_extensions-4.6.3:\n      Successfully uninstalled typing_extensions-4.6.3\n  Attempting uninstall: tomlkit\n    Found existing installation: tomlkit 0.12.1\n    Uninstalling tomlkit-0.12.1:\n      Successfully uninstalled tomlkit-0.12.1\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.27.0\n    Uninstalling starlette-0.27.0:\n      Successfully uninstalled starlette-0.27.0\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.6.3\n    Uninstalling pydantic_core-2.6.3:\n      Successfully uninstalled pydantic_core-2.6.3\n  Attempting uninstall: typer\n    Found existing installation: typer 0.9.0\n    Uninstalling typer-0.9.0:\n      Successfully uninstalled typer-0.9.0\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.9\n    Uninstalling pydantic-1.10.9:\n      Successfully uninstalled pydantic-1.10.9\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.16.4\n    Uninstalling huggingface-hub-0.16.4:\n      Successfully uninstalled huggingface-hub-0.16.4\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.98.0\n    Uninstalling fastapi-0.98.0:\n      Successfully uninstalled fastapi-0.98.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbotocore 1.31.17 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.2 which is incompatible.\nchex 0.1.82 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ngoogle-auth 2.20.0 requires urllib3<2.0, but you have urllib3 2.2.2 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.0.1 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\nspacy 3.6.1 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.4 which is incompatible.\nydata-profiling 4.3.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.8.2 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fastapi-0.112.1 ffmpy-0.4.0 gradio-4.41.0 gradio-client-1.3.0 httpcore-1.0.5 httpx-0.27.0 huggingface-hub-0.24.5 pydantic-2.3.0 pydantic-core-2.20.1 python-multipart-0.0.9 ruff-0.6.1 semantic-version-2.10.0 starlette-0.38.2 tomlkit-0.12.0 typer-0.12.4 typing-extensions-4.12.2 urllib3-1.26.16\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install Gradio (uncomment if needed)\n# !pip install gradio\n\nimport gradio as gr\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# # Load your model and tokenizer\n# model_name = \"your-model-name\"  # Replace with your model name\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n\n# Define the function that will generate the response using your model\ndef generate_response(user_input):\n    messages = [{\"role\": \"user\", \"content\": user_input}]\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n    outputs = model.generate(**inputs, max_length=150, num_return_sequences=1)\n\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    response = text.split(\"assistant\", 1)[-1].strip()\n\n    return response\n\n# Create the Gradio interface\ndemo = gr.Interface(\n    fn=generate_response,  # The function that processes the input and returns the output\n    inputs=\"text\",         # The input type (text box)\n    outputs=\"text\",        # The output type (text box)\n    title=\"Your Model Chatbot\",  # Title of the interface\n    description=\"This chatbot interacts with users based on a language model.\",  # Description\n)\n\n# Launch the interface\ndemo.launch()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:44:45.355264Z","iopub.execute_input":"2024-08-18T16:44:45.355556Z","iopub.status.idle":"2024-08-18T16:44:57.849443Z","shell.execute_reply.started":"2024-08-18T16:44:45.355527Z","shell.execute_reply":"2024-08-18T16:44:57.848564Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\nRunning on public URL: https://7aac04bf1694e4d2d1.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://7aac04bf1694e4d2d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}]},{"cell_type":"code","source":"# # !pip install gradio\n# import gradio as gr\n\n\n# def greet(name):\n#     return \"Hello \" + name\n\n\n# demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n\n# demo.launch()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:35:28.077100Z","iopub.execute_input":"2024-08-18T16:35:28.077501Z","iopub.status.idle":"2024-08-18T16:35:31.429357Z","shell.execute_reply.started":"2024-08-18T16:35:28.077470Z","shell.execute_reply":"2024-08-18T16:35:31.428436Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\nRunning on public URL: https://ee166e773c4de562d1.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://ee166e773c4de562d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}